{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 1 - Clasificación de texto según el autor\n",
    "Construid dos modelos de diferentes autores: Josep Carner y Miquel dels Sants Oliver. Para ello, podéis usar sus obras disponibles en el Proyecto Gutenberg. Por ejemplo, de Carner, tomad la traducción de los cuentos de Mark Twain. Luego, clasificad frases en el estilo de cada uno que muestren cómo vuestro modelo las identifica correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils import open_data\n",
    "from text import *\n",
    "\n",
    "hostal = open_data(\"CA-text/hostal.txt\").read()\n",
    "wordseq = words(hostal)\n",
    "\n",
    "P_Miquel = UnigramWordModel(wordseq, 5)\n",
    "\n",
    "adventures = open_data(\"CA-text/adventures.txt\").read()\n",
    "wordseq = words(adventures)\n",
    "\n",
    "P_Josep = UnigramWordModel(wordseq, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidramirez/Documents/IEDIB/02 Modelos IA/T5 NLP/.venv/lib/python3.10/site-packages/qpsolvers/solvers/__init__.py:752: UserWarning: no QP solver found on your system, you can install solvers from PyPI by ``pip install qpsolvers[open_source_solvers]``\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from learning import NaiveBayesLearner\n",
    "\n",
    "dist = {('Miquel', 1): P_Miquel, ('Josep', 1): P_Josep}\n",
    "\n",
    "nBS = NaiveBayesLearner(dist, simple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize(sentence, nBS):\n",
    "    sentence = sentence.lower()\n",
    "    sentence_words = words(sentence)\n",
    "    \n",
    "    return nBS(sentence_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero he probado a usar una frase extraída de los archivos CA-text/hostal.txt y CA-text/adventures.txt para comprobar si el modelo es capaz de identificar correctamente el autor de la frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miquel'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize(\"havia donat la direcció a un personatge destinat a fer molt de renou\", nBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Josep'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize(\"potser mai no bastarà mentre el món sigui món\", nBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar con otro ejemplo, le he preguntado al ChatGPT la diferencia de estilo entre los dos autores. A continuación, se muestra la respuesta del ChatGPT:\n",
    "\n",
    "```\n",
    "Diferencias resumidas:\n",
    "\t•\tJosep Carner → Poètic, refinat, musical i idealitzat.\n",
    "\t•\tMiquel dels Sants Oliver → Narratiu, directe, reflexiu i analític.\n",
    "```\n",
    "\n",
    "A raíz de esto he probado a clasificar dos frases de cada autor para ver si el modelo es capaz de diferenciar el estilo de cada uno. Pero en este caso estas frases no han sido sacadas de los archivios .txt sino generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miquel'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize(\"Els carrers, encara humits, semblaven desertar de la nit\", nBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Josep'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize(\"Amb un gest majestuós em fità un instant\", nBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 2 - Generar texto\n",
    "\n",
    "Construye modelos n-gram con n=1, n=3, n=5 y n=7 a partir de un texto de tu elección. Puede ser una obra del Proyecto Gutenberg o una noticia de prensa, por ejemplo. Observa cómo, a medida que aumenta el número de palabras consideradas, la plausibilidad del texto generado mejora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He optado por elegir Don Quijote de la Mancha como corpus de entranmiento para el modelo. El texto está guardado en /aima-data/ES-text/quijote.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from text import NgramWordModel\n",
    "\n",
    "def generar_texto(model, n, longitud):\n",
    "    generated_text = []\n",
    "    # Elegimos una semilla aleatoria del modelo\n",
    "    seed = random.choice(list(model.dictionary.keys()))\n",
    "    generated_text.extend(seed)\n",
    "    \n",
    "    for i in range(longitud - n):\n",
    "        last_sequence = tuple(generated_text[-(n-1):])  # Tomamos los últimos caracteres generados\n",
    "        next_possibilities = []  # Lista para guardar opciones\n",
    "\n",
    "        for k in model.dictionary.keys():  # Revisamos todos los n-gramas aprendidos\n",
    "            if k[:n-1] == last_sequence:  # Si el inicio del n-grama coincide...\n",
    "                next_possibilities.append(k[n-1])  # Guardamos la siguiente letra\n",
    "        \n",
    "        if not next_possibilities:  # Si no hay opciones, salimos\n",
    "            break\n",
    "\n",
    "        next = random.choice(next_possibilities)  # Elegimos una opción aleatoria\n",
    "        generated_text.append(next)  # La añadimos al texto generado\n",
    "    \n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# Cargar texto y entrenar el modelo\n",
    "quijote = open_data(\"ES-text/quijote.txt\").read()\n",
    "wordseq = words(quijote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez preparada la función hago distintas pruebas modificando la n. Y veremos que es correcta la afirmación anterior; a medida que aumentamos el número de carácteres que se tienen en cuenta el texto generado empieza a tener más sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el cansancio deste combate o ya vengan encaminadas por la empresa y fue luego escrito al duque de austria digan que esto hab a habiendo primero con groseras ceremonias rogado a don antonio y pregunt ndose los unos de contento mira hermano cuando yo pens que todos le dijeron que\n"
     ]
    }
   ],
   "source": [
    "model_ngram = NgramWordModel(3, wordseq)\n",
    "\n",
    "# Generar texto\n",
    "new_text = generar_texto(model_ngram, 3, 50)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nadie porque as como maritornes le at ella y la otra se fueron muertas de risa y le dejaron asido de manera que fue imposible soltarse estaba pues como se ha dicho castillos eran a su parecer todas las ventas donde alojaba y que la hija del ventero no s\n"
     ]
    }
   ],
   "source": [
    "model_ngram = NgramWordModel(5, wordseq)\n",
    "\n",
    "# Generar texto\n",
    "new_text = generar_texto(model_ngram, 5, 50)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que hab a sabido que en aquella casa viv a nos deb a de haber hecho aquel beneficio y en se al de que lo agradec amos hecimos zalemas a uso de moros inclinando la cabeza doblando el cuerpo y poniendo los brazos sobre el pecho de all a poco\n"
     ]
    }
   ],
   "source": [
    "model_ngram = NgramWordModel(7, wordseq)\n",
    "\n",
    "# Generar texto\n",
    "new_text = generar_texto(model_ngram, 7, 50)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por hacer una prueba con una n mucho más grande he decido usar 30. Vemos que da un buen resultado al tener más contexto.  \n",
    "Y a pesar de que algunas frases de las que genera no tienen un sentido a nivel de significado, por lo menos sí que son correctas en gran mayoría gramaticalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d as ha que pudiera yo estar bogando en ellas no son los amores como los que vuestra merced piensa dijo el galeote que los m os fueron que quise tanto a una canasta de colar atestada de ropa blanca que la abrac conmigo tan fuertemente que a no quit\n"
     ]
    }
   ],
   "source": [
    "model_ngram = NgramWordModel(30, wordseq)\n",
    "\n",
    "# Generar texto\n",
    "new_text = generar_texto(model_ngram, 30, 50)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 3 - Análisis sintáctico\n",
    "\n",
    "Aplicad las herramientas de alguna libería de Python para analizar sintácticamente la oración siguiente:  \n",
    "\n",
    "```\n",
    "Tots els éssers humans neixen lliures i iguals en dignitat i drets.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sintaxis:\n",
      "Frases nominales: ['Tots els éssers humans', 'dignitat i drets']\n",
      "Verbos: ['neixar']\n",
      "Sujetos: ['éssers']\n",
      "Adjetivos: ['humans', 'lliures', 'iguals']\n",
      "Preposiciones: ['en']\n",
      "Conjunciones: ['i', 'i']\n",
      "\n",
      "Información de cada palabra:\n",
      "Palabra: Tots | Lema: tot | Etiqueta gramatical: DET | Dependencia sintáctica: det | Gobernante: els \n",
      "\n",
      "Palabra: els | Lema: el | Etiqueta gramatical: DET | Dependencia sintáctica: det | Gobernante: éssers \n",
      "\n",
      "Palabra: éssers | Lema: ésser | Etiqueta gramatical: NOUN | Dependencia sintáctica: nsubj | Gobernante: neixen \n",
      "\n",
      "Palabra: humans | Lema: humà | Etiqueta gramatical: ADJ | Dependencia sintáctica: amod | Gobernante: éssers \n",
      "\n",
      "Palabra: neixen | Lema: neixar | Etiqueta gramatical: VERB | Dependencia sintáctica: ROOT | Gobernante: neixen \n",
      "\n",
      "Palabra: lliures | Lema: lliure | Etiqueta gramatical: ADJ | Dependencia sintáctica: obj | Gobernante: neixen \n",
      "\n",
      "Palabra: i | Lema: i | Etiqueta gramatical: CCONJ | Dependencia sintáctica: cc | Gobernante: iguals \n",
      "\n",
      "Palabra: iguals | Lema: igual | Etiqueta gramatical: ADJ | Dependencia sintáctica: conj | Gobernante: lliures \n",
      "\n",
      "Palabra: en | Lema: en | Etiqueta gramatical: ADP | Dependencia sintáctica: case | Gobernante: dignitat \n",
      "\n",
      "Palabra: dignitat | Lema: dignitat | Etiqueta gramatical: NOUN | Dependencia sintáctica: nmod | Gobernante: lliures \n",
      "\n",
      "Palabra: i | Lema: i | Etiqueta gramatical: CCONJ | Dependencia sintáctica: cc | Gobernante: drets \n",
      "\n",
      "Palabra: drets | Lema: dret | Etiqueta gramatical: NOUN | Dependencia sintáctica: conj | Gobernante: dignitat \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de lenguaje en catalán\n",
    "nlp = spacy.load(\"ca_core_news_sm\")\n",
    "\n",
    "# Frase a analizar\n",
    "sentence = \"Tots els éssers humans neixen lliures i iguals en dignitat i drets\"\n",
    "\n",
    "# Analizar la frase\n",
    "doc = nlp(sentence)\n",
    "\n",
    "\n",
    "# Analizar la sintaxis de la frase\n",
    "print(\"Sintaxis:\")\n",
    "print(\"Frases nominales:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbos:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "print(\"Sujetos:\", [token.text for token in doc if token.dep_ == \"nsubj\"])\n",
    "print(\"Adjetivos:\", [token.text for token in doc if token.pos_ == \"ADJ\"])\n",
    "print(\"Preposiciones:\", [token.text for token in doc if token.pos_ == \"ADP\"])\n",
    "print(\"Conjunciones:\", [token.text for token in doc if token.pos_ == \"CCONJ\"])\n",
    "print()\n",
    "\n",
    "\n",
    "# Mostrar información de cada palabra\n",
    "print(\"Información de cada palabra:\")\n",
    "for token in doc:\n",
    "    print(f\"Palabra: {token.text} | Lema: {token.lemma_} | Etiqueta gramatical: {token.pos_} | Dependencia sintáctica: {token.dep_} | Gobernante: {token.head.text} \\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 4 - Recuperación de información\n",
    "\n",
    "#### 1.\tCargad el artículo de la Wikipedia sobre Europa (https://ca.wikipedia.org/wiki/Europa) en una lista de frases y recuperad la siguiente información, buscando las frases más similares:\n",
    "\n",
    "- Quan es va gestar el concepte d'Europa?\n",
    "- Quina és l'espècie humana autòctona d'Europa?\n",
    "- Quan es varen formar els estats actuals d'Europa?\n",
    "- Quin clima té Europa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para completar el ejercicio usaré \"requests\" para obtener el contenido de la página y \"BeautifulSoup\" para extraer el texto de la página.\n",
    "Con \"spacy\" analizaré las frases y buscaré las más similares a las preguntas usando transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pregunta: Quan es va gestar el concepte d'Europa?\n",
      "Frase más similar: El naixement d'Europa[modifica]\n",
      "Europa, com a unitat històrica, es va gestar a l'edat mitjana.\n",
      "\n",
      "Pregunta: Quina és l'espècie humana autòctona d'Europa?\n",
      "Frase más similar: Aquesta espècie es trobava ja a Europa quan va arribar l'humà de Cromanyó (Homo sapiens), espècie a què pertany tota la humanitat actual.\n",
      "\n",
      "Pregunta: Quan es varen formar els estats actuals d'Europa?\n",
      "Frase más similar: Molts dels estats de l'Europa actual es van formar després de la Primera Guerra Mundial.\n",
      "\n",
      "Pregunta: Quin clima té Europa?\n",
      "Frase más similar: [Consulta: 14 febrer 2011].\n",
      "\n",
      "↑ «European Climate».\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "url = \"https://ca.wikipedia.org/wiki/Europa\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraer el texto del artículo\n",
    "article_text = ' '.join([p.text for p in soup.find_all([\"p\", \"div\"]) if p.text])\n",
    "\n",
    "# Dividir en frases\n",
    "nlp = spacy.load(\"ca_core_news_sm\")\n",
    "doc = nlp(article_text)\n",
    "sentences = [sent.text for sent in doc.sents] \n",
    "\n",
    "questions = [\n",
    "  \"Quan es va gestar el concepte d'Europa?\", \n",
    "  \"Quina és l'espècie humana autòctona d'Europa?\", \n",
    "  \"Quan es varen formar els estats actuals d'Europa?\", \n",
    "  \"Quin clima té Europa?\"\n",
    "]\n",
    "\n",
    "# Cargar modelo de embeddings para comparar\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Calcular similitud entre preguntas y frases del artículo\n",
    "question_embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    similarities = util.pytorch_cos_sim(question_embeddings[i], sentence_embeddings)\n",
    "    best_idx = similarities.argmax()\n",
    "    print(f\"\\nPregunta: {question}\")\n",
    "    print(f\"Frase más similar: {sentences[best_idx]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.\t¿Todas las preguntas tienen una respuesta adecuada? ¿Hay preguntas que convendría formular de otra manera?\n",
    "\n",
    "Vemos que la mayoría de las preguntas tienen una respuesta bastante correcta. De hecho desde la primera hasta la tercera nos responde de manera perfecta.\n",
    "El problema está en la última pregunta, parece que no acaba de dar con la respuesta que toca y simplemente nos da una referencia a un link que habla del clima.\n",
    "\n",
    "Vamos a probar de formular la pregunta de manera distitna para ver si obtenemos una mejor respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pregunta: Com es el clima a Europa?\n",
      "Frase más similar: [Consulta: 14 febrer 2011].\n",
      "\n",
      "↑ «European Climate».\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "  \"Com es el clima a Europa?\",\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "question_embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    similarities = util.pytorch_cos_sim(question_embeddings[i], sentence_embeddings)\n",
    "    best_idx = similarities.argmax()\n",
    "    print(f\"\\nPregunta: {question}\")\n",
    "    print(f\"Frase más similar: {sentences[best_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiar manera de preguntar sobre el clima parece no modificar la respuesta. Para confirmarlo voy a preguntar dando la respuesta implícita en la pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pregunta: Europa té un clima mità temperat?\n",
      "Frase más similar: [Consulta: 14 febrer 2011].\n",
      "\n",
      "↑ «European Climate».\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "  \"Europa té un clima mità temperat?\",\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "question_embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    similarities = util.pytorch_cos_sim(question_embeddings[i], sentence_embeddings)\n",
    "    best_idx = similarities.argmax()\n",
    "    print(f\"\\nPregunta: {question}\")\n",
    "    print(f\"Frase más similar: {sentences[best_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero después de investgiar cuál podría ser una manera de mejorar la respuesta me doy cuenta que la respuesta que obtenemos ahora es parte de las \"Referencias\" de wikipedia, al final de la web.\n",
    "Esto no nos interesa ya que no contiene información relevante, por lo que se me ocurre filtrar el texto que extraemos de la web para que no contenga frases si hay carácteres entre \"«»\".\n",
    "De esta manera evito que se devuelva información de las referencias.\n",
    "\n",
    "Vamos a probarlo, y además usando la pregunta original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pregunta: Quin clima té Europa?\n",
      "Frase más similar: El clima d'Europa és de naturalesa temperada i continental amb un clima marítim que predomina a les costes occidentals i un clima mediterrani al sud.\n"
     ]
    }
   ],
   "source": [
    "sentences = [sent.text for sent in doc.sents if not any(char in sent.text for char in \"«»\")]\n",
    "\n",
    "questions = [\n",
    "  \"Quin clima té Europa?\",\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "question_embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    similarities = util.pytorch_cos_sim(question_embeddings[i], sentence_embeddings)\n",
    "    best_idx = similarities.argmax()\n",
    "    print(f\"\\nPregunta: {question}\")\n",
    "    print(f\"Frase más similar: {sentences[best_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmamos que así obtenemos la respuesta correcta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
